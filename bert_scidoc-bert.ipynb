{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT and XLNet to do document classification\n",
    "This notebook uses the transfomer library and, on top of that,  simpletransformers.\n",
    "\n",
    "The first part loads the arxiv science docs data and formats them into the pandas tables needed to do the training and testing.\n",
    "\n",
    "After that we do the classisification with BERT.  While it loads the small language model automatically, You will need to train the network first with the arxiv data.\n",
    "after it has been trained you can reload the model if you want to play with it later.\n",
    "\n",
    "The next part does the classification with XLNET.   the process is identical to the training and testing step with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data.   \n",
    "you can find the data in the same github repo where you found this notebook.   it is not very big.   This data loader is at all general.  it is specific to this collection, so don't bother to understand this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(path, name):\n",
    "    filename = path+name+\".p\"\n",
    "    fileobj = open(filename, \"rb\")\n",
    "    z = fileobj.read()\n",
    "    lst = pickle.loads(z)\n",
    "    titles = []\n",
    "    sitenames = []\n",
    "    abstracts = []\n",
    "    for i in range(0, len(lst)):\n",
    "        titles.extend([lst[i][0]])\n",
    "        sitenames.extend([lst[i][1]])\n",
    "        abstracts.extend([lst[i][2]])\n",
    "        \n",
    "    print(\"done loading \"+filename)\n",
    "    return abstracts, sitenames, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts, sitenames, disp_title = load_docs(\"./sci_doc/\", \n",
    "                                     \"sciml_data_arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(subj, basepath):\n",
    "    docpath =basepath+ \"/config_\"+subj+\".json\"\n",
    "    with open(docpath, 'rb') as f:\n",
    "        doc = f.read() \n",
    "    z =json.loads(doc)\n",
    "    subject = z['subject']\n",
    "    loadset = z['loadset']\n",
    "    subtopics = []\n",
    "    for w in z['supertopics']:\n",
    "        subtopics.extend([(w[0], w[1])])\n",
    "    return subject, loadset, subtopics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(subtopics):\n",
    "    dic = {}\n",
    "    for main in subtopics:\n",
    "        sl = main[1]\n",
    "        for x in sl:\n",
    "            dic[x] = main[0]\n",
    "    return dic\n",
    "\n",
    "def split_titles(disp_title):\n",
    "    subject,loadset, subtopics = read_config(\"all4\",\"./sci_doc\")\n",
    "    dic = make_dict(subtopics)\n",
    "    lis = []\n",
    "    for ti in disp_title:\n",
    "        l = ti.find('[')\n",
    "        if(l >= 0):\n",
    "            #lis.append(ti[:l])\n",
    "            e = ti[l+1:]\n",
    "            l2 = e.find(']')\n",
    "            e = e[:l2]\n",
    "            try:\n",
    "                if dic[e]== 'compsci':\n",
    "                    lis.append([ti[:l], 0, e])\n",
    "                if dic[e]== 'math':\n",
    "                    lis.append([ti[:l], 1, e])\n",
    "                if dic[e]== 'Physics':\n",
    "                    lis.append([ti[:l], 2, e])\n",
    "                if dic[e]== 'bio':\n",
    "                    lis.append([ti[:l], 3, e])\n",
    "                if dic[e]== 'finance':\n",
    "                    lis.append([ti[:l], 4, e])\n",
    "            except:\n",
    "                print(e)\n",
    "            \n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject,loadset, subtopics = read_config(\"all4\",\"./sci_doc\")\n",
    "dic = make_dict(subtopics)\n",
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(disp_title, abstracts):\n",
    "    subject,loadset, subtopics = read_config(\"all4\",\"./sci_doc\")\n",
    "    dic = make_dict(subtopics)\n",
    "    lis = []\n",
    "    ind = 0\n",
    "    for ind in range(len(disp_title)): #disp_title,titles:\n",
    "        ti = disp_title[ind]\n",
    "        te = abstracts[ind]\n",
    "        l = ti.find('[')\n",
    "        if(l >= 0):\n",
    "            #lis.append(ti[:l])\n",
    "            e = ti[l+1:]\n",
    "            l2 = e.find(']')\n",
    "            e = e[:l2]\n",
    "            try:\n",
    "                if dic[e]== 'compsci':\n",
    "                    lis.append([te, 0])\n",
    "                if dic[e]== 'math':\n",
    "                    lis.append([te, 1])\n",
    "                if dic[e]== 'Physics':\n",
    "                    lis.append([te, 2])\n",
    "                if dic[e]== 'bio':\n",
    "                    lis.append([te, 3])\n",
    "                if dic[e]== 'finance':\n",
    "                    lis.append([te, 4])\n",
    "            except:\n",
    "                print(e)\n",
    "            \n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titles = split_titles(disp_title[0:4500]) #contains titles only\n",
    "eval_titles  = split_titles(disp_title[4500:])\n",
    "train_text = split_text(disp_title[0:4500], abstracts[0:4500])\n",
    "eval_text = split_text(disp_title[4500:],abstracts[4500:])  #contains text + class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_titles[0])\n",
    "print(eval_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluation data needs to be in a Pandas Dataframe containing at least two columns. #If the Dataframe has a header, it should contain a 'text' and a 'labels' column. \n",
    "#If no header is present, the Dataframe should contain at least two columns, \n",
    "#with the first column is the text with type str, \n",
    "#and the second column in the label with type int.\n",
    "#train_data = [['Example sentence belonging to class 1', 1], \n",
    "#              ['Example sentence belonging to class 0', 0], \n",
    "#              ['Example eval senntence belonging to class 2', 2]]\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "#eval_data = [['Example eval sentence belonging to class 1', 1], \n",
    "#             ['Example eval sentence belonging to class 0', 0], \n",
    "#             ['Example eval senntence belonging to class 2', 2]]\n",
    "eval_df = pd.DataFrame(eval_titles)\n",
    "text_df = pd.DataFrame(train_text)\n",
    "text_eval_df = pd.DataFrame(eval_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do BERT classification.  \n",
    "the first time you do this you need to run the trainer.   after that is done move the output to a seperate directory \"outputs-bert-originalall4\".   after you do that you can later reload the data as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#either load the previously trained model \n",
    "#model = ClassificationModel('bert',  'outputs-bert-origianlall4/', num_labels=5, use_cuda=False)\n",
    "\n",
    "# or create and train it\n",
    "model = ClassificationModel('bert', 'bert-base-cased', num_labels=5, \n",
    "                            args={'reprocess_input_data': True, \n",
    "                                  'overwrite_output_dir': True}, use_cuda=False)\n",
    "\n",
    "model.train_model(text_df)\n",
    "#this takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ''\n",
    "model_outputs = []\n",
    "wrong_predictions = []\n",
    "print(len(text_eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(text_eval_df[0:2600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fraction wrong = \", len(wrong_predictions)/2600.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {0: 'cs  ', 1: 'math', 2: 'phys', 3: 'bio', 4: 'fina'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell just selects the first few incorrect predictions and prints them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "cs_sample = True\n",
    "math_sample= True\n",
    "physics_sampe = True\n",
    "bio_sample = True\n",
    "finance_sampe = True\n",
    "need_sample = [True, True, True, True, True]\n",
    "for s in range(len(wrong_predictions)):\n",
    "    if need_sample[wrong_predictions[s].label]:\n",
    "            print('item',wrong_predictions[s].guid, \n",
    "              'is ',category[wrong_predictions[s].label], ' but predicted to be ',\n",
    "              category[model_outputs[wrong_predictions[s].guid].argmax()])\n",
    "            print(eval_titles[wrong_predictions[s].guid])\n",
    "            print(eval_text[wrong_predictions[s].guid])\n",
    "            print('------------------------------------------------------------')\n",
    "            need_sample[wrong_predictions[s].label] = False\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "this is the output to expect from the line above.\n",
    "item 0 is  math  but predicted to be  cs  \n",
    "['Joint estimation and model order selection for one dimensional ARMA   models via convex optimization: a nuclear norm penalization approach ', 1, 'math.ST']\n",
    "['The problem of estimating ARMA models is computationally interesting due to the nonconcavity of the log-likelihood function. Recent results were based on the convex minimization. Joint model selection using penalization by a convex norm, e.g. the nuclear norm of a certain matrix related to the state space formulation was extensively studied from a computational viewpoint. The goal of the present short note is to present a theoretical study of a nuclear norm penalization based variant of the method of \\\\cite{Bauer:Automatica05,Bauer:EconTh05} under the assumption of a Gaussian noise process.', 1]\n",
    "------------------------------------------------------------\n",
    "item 1 is  cs    but predicted to be  math\n",
    "['Learning Single Index Models in High Dimensions ', 0, 'stat.ML']\n",
    "['Single Index Models (SIMs) are simple yet flexible semi-parametric models for classification and regression. Response variables are modeled as a nonlinear, monotonic function of a linear combination of features. Estimation in this context requires learning both the feature weights, and the nonlinear function. While methods have been described to learn SIMs in the low dimensional regime, a method that can efficiently learn SIMs in high dimensions has not been forthcoming. We propose three variants of a computationally and statistically efficient algorithm for SIM inference in high dimensions. We establish excess risk bounds for the proposed algorithms and experimentally validate the advantages that our SIM learning methods provide relative to Generalized Linear Model (GLM) and low dimensional SIM based learning methods.', 0]\n",
    "------------------------------------------------------------\n",
    "item 44 is  phys  but predicted to be  math\n",
    "['Effect of marital status on death rates. Part 1: High accuracy   exploration of the Farr-Bertillon effect ', 2, 'physics.soc-ph']\n",
    "['The Farr-Bertillon law says that for all age-groups the death rate of married people is lower than the death rate of people who are not married (i.e. single, widowed or divorced). Although this law has been known for over 150 years, it has never been established with great accuracy. This even let some authors argue that it was a statistical artefact. It is true that the data must be selected and analyzed with great care, especially for age groups of small size such as widowers under 25. The observations reported in this paper were selected and designed in the same way as experiments in physics, that is to say with the objective of minimizing the error bars for all age-groups. It will be seen that data appropriate for mid-age groups may be unsuitable for young age groups and vice versa. The investigation led to the following results. (1) The FB effect is basically the same for men and women, except that on average it is about 20\\\\% stronger for men. (2) There is a marked difference between single or divorced persons on the one hand, for whom the effect is largest around the age of 45, and widowed persons on the other hand, for whom the effect is largest around the age of 25. (3) When different causes of death are distinguished, the effect is largest for suicide and smallest for cancer. (4) For young widowers the death rates are up to 10 times higher than for married persons of same age. This extreme form of the FB effect will be referred to as the \"young widower effect.\" A possible connection between the FB effect and Martin Raff\\'s \"Stay alive\" effect for cells in an organism is discussed in the last section.', 2]\n",
    "------------------------------------------------------------\n",
    "item 108 is  bio  but predicted to be  cs  \n",
    "['A new framework for Euclidean summary statistics in neural spike train   space ', 3, 'q-bio.NC']\n",
    "['Statistical analysis and inference on spike trains is one of the central topics in the neural coding. It is of great interest to understand the underlying structure of given neural data. Based on the metric distances between spike trains, recent investigations have introduced the notion of average or prototype spike train to characterize the template pattern in the neural activity. However, as those metrics lack certain Euclidean properties, the defined averages are non-unique, and do not share the conventional properties of a mean. In this article, we propose a new framework to define the mean spike train where we adopt an Euclidean-like metric from an L p family. We demonstrate that this new mean spike train properly represents the average pattern in the conventional fashion, and can be effectively computed using a theoretically-proven convergent procedure. We compare this mean with other spike train averages and demonstrate its superiority. Furthermore, we apply the new framework in a recording from rodent geniculate ganglion, where background firing activity is a common is- sue for neural coding. We show that the proposed mean spike train can be utilized to remove the background noise and improve decoding performance.', 3]\n",
    "------------------------------------------------------------\n",
    "item 819 is  fina  but predicted to be  phys\n",
    "['Detrended partial cross-correlation analysis of two time series   influenced by common external forces ', 4, 'q-fin.ST']\n",
    "['We propose a new method, detrended partial cross-correlation analysis (DPXA), to uncover the intrinsic power-law cross-correlations between two simultaneously recorded time series in the presence of nonstationarity after removing the effects of other time series acting as common forces. The DPXA method is a generalization of the detrended cross-correlation analysis by taking into account the partial correlation analysis. We illustrate the performance of the method using bivariate fractional Brownian motions and multifractal binomial measures with analytical expressions and apply it to extract the intrinsic cross-correlation between crude oil and gold futures by considering the impact of the US dollar index.', 4]\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function prints out the confusion matrix.   For row X it prints the percent of predictions it makes in each category.  In other words if an X abstract is predicted to be a Y, then a 1 is added to row X, column Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion(data, model_outputs):\n",
    "    pr = []\n",
    "    tr = []\n",
    "    for ou in model_outputs:\n",
    "        pr.append(ou.argmax())\n",
    "    for x in data:\n",
    "        #print(x)\n",
    "        tr.append(x[1])\n",
    "    mat = np.zeros([5,5])\n",
    "    i = 0\n",
    "    for p in pr:\n",
    "        mat[tr[i], p]+= 1\n",
    "        i+=1\n",
    "        \n",
    "    truevals = mat[0,0]+mat[1,1]+mat[2,2]+mat[3,3]+mat[4,4]\n",
    "    for i in range(5):\n",
    "        s = np.sum(mat[i,:])\n",
    "        mat[i,:] = np.round(100*mat[i,:]/s)\n",
    "    pds = {' ':['compsci','math', 'physics','bio','finance']}\n",
    "    pds['compsci'] = mat[:,0]\n",
    "    pds['math']    = mat[:,1]\n",
    "    pds['physics'] = mat[:,2]\n",
    "    pds['bio']    = mat[:,3]\n",
    "    pds['finance'] = mat[:,4]\n",
    "    print(\"accuracy =\", truevals/len(data))\n",
    "    #print(pds)\n",
    "    pdf = pd.DataFrame(pds)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_confusion(eval_text[0:2600], model_outputs))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accuracy = 0.7615384615384615\n",
    "            compsci  math  physics   bio  finance\n",
    "0  compsci     81.0  12.0      3.0   3.0      1.0\n",
    "1     math     24.0  57.0      7.0   7.0      5.0\n",
    "2  physics      5.0   3.0     88.0   4.0      0.0\n",
    "3      bio     13.0   1.0     12.0  73.0      0.0\n",
    "4  finance      0.0   1.0      2.0   0.0     97.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLNET\n",
    "now we will try  to  do the classification using xlnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as with bert, either read the trained model\n",
    "#modelxl = ClassificationModel('xlnet',  'outputs-xlnet-origianall4/', num_labels=5, use_cuda=False)\n",
    "#or trainit\n",
    "modelxl = ClassificationModel('xlnet', 'xlnet-base-cased', num_labels=5, \n",
    "                            args={'reprocess_input_data': True, \n",
    "                                 'overwrite_output_dir': True}, use_cuda=False)\n",
    "modelxl.train_model(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultxl, model_outputslx, wrong_predictionslx = modelxl.eval_model(text_eval_df[0:2600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_confusion(eval_text[0:2600], model_outputslx))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "this is xlnet on eval text 0:2600\n",
    "accuracy = 0.7680769230769231\n",
    "            compsci  math  physics   bio  finance\n",
    "0  compsci     78.0  10.0      3.0   6.0      2.0\n",
    "1     math     21.0  59.0      7.0   8.0      5.0\n",
    "2  physics      3.0   4.0     89.0   4.0      0.0\n",
    "3      bio      4.0   1.0     19.0  76.0      0.0\n",
    "4  finance      2.0   2.0      2.0   0.0     94.0\n",
    "\n",
    "if you comapare this to Bert on the same data:\n",
    "\n",
    "accuracy = 0.7615384615384615\n",
    "            compsci  math  physics   bio  finance\n",
    "0  compsci     81.0  12.0      3.0   3.0      1.0\n",
    "1     math     24.0  57.0      7.0   7.0      5.0\n",
    "2  physics      5.0   3.0     88.0   4.0      0.0\n",
    "3      bio     13.0   1.0     12.0  73.0      0.0\n",
    "4  finance      0.0   1.0      2.0   0.0     97.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute the \"best of 2\"\n",
    "this function takes the two most higly rated signals and if one of them is correct then the it is considered correct.  of course, this is a rather ah hoc measure, but still interesting.  see the analysis in the paper: https://esciencegroup.com/2020/02/20/modeling-natural-language-with-transformers-bert-roberta-and-xlnet/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestof2(model_outputs, text):\n",
    "    mat = np.zeros([5,5])\n",
    "    for i in range(len(model_outputs)):\n",
    "        out = np.zeros(5)\n",
    "        for j in range(5):\n",
    "            out[j] = model_outputs[i][j]\n",
    "        v = out.argmax()\n",
    "        out[v] = -3.14       \n",
    "        u = out.argmax()\n",
    "        if u > 0.25*v:\n",
    "            second = u\n",
    "        else:\n",
    "            second = -1\n",
    "        tru = text[i][1]\n",
    "        \n",
    "        if (v != tru) and (second == tru):\n",
    "            v = second\n",
    "        mat[tru, v]+=1\n",
    "        #print(i, tru, v, out)\n",
    "    truevals = mat[0,0]+mat[1,1]+mat[2,2]+mat[3,3]+mat[4,4]\n",
    "    for i in range(5):\n",
    "        s = np.sum(mat[i,:])\n",
    "        mat[i,:] = np.round(100*mat[i,:]/s)\n",
    "    pds = {' ':['compsci', 'math', 'physics','bio','finance']}\n",
    "    pds['compsci'] = mat[:,0]\n",
    "    pds['math']    = mat[:,1]\n",
    "    pds['physics'] = mat[:,2]\n",
    "    pds['bio']    = mat[:,3]\n",
    "    pds['finance'] = mat[:,4]\n",
    "    print(\"accuracy =\", truevals/len(text))\n",
    "    pdf = pd.DataFrame(pds) \n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestof2(model_outputs,  eval_text[0:2600]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "this is xlnet with best of two score.  \n",
    "accuracy = 0.8880769230769231\n",
    "            compsci  math  physics   bio  finance\n",
    "0  compsci     78.0  10.0      3.0   6.0      2.0\n",
    "1     math      2.0  87.0      1.0   5.0      5.0\n",
    "2  physics      2.0   1.0     96.0   1.0      0.0\n",
    "3      bio      2.0   1.0      6.0  90.0      0.0\n",
    "4  finance      2.0   0.0      2.0   0.0     96.0\n",
    "\n",
    "this is bert with the best of two score\n",
    "accuracy = 0.8919230769230769\n",
    "            compsci  math  physics   bio  finance\n",
    "0  compsci     81.0  12.0      3.0   3.0      1.0\n",
    "1     math      5.0  85.0      2.0   4.0      5.0\n",
    "2  physics      1.0   1.0     96.0   1.0      0.0\n",
    "3      bio      4.0   1.0      4.0  90.0      0.0\n",
    "4  finance      0.0   1.0      2.0   0.0     97.0\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
